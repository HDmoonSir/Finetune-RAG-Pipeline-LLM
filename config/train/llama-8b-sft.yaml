# configs/train/llama-8b-sft.yaml

# General settings
experiment_name: "llama-8b-sft"
output_dir: "exp_results"
seed: 42

# Model settings
model:
  base_model_id: "MLP-KTLim/llama-3-Korean-Bllossom-8B"
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  unsupervised_lora_path: "exp_result/your_path" # Optional: Path to unsupervised LoRA for pre-merge before SFT

# Training settings
# mode(sft, unsupervised)
training:
  mode: "unsupervised"
  use_unsloth: false
  dataset_path: "data_result/gemini_generated_qa_dataset.jsonl"
  max_seq_length: 2048
  num_epochs: 3
  batch_size: 1
  grad_accum_steps: 4
  optimizer: "adamw_torch"
  learning_rate: 2e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  logging_steps: 25
  save_steps: 250
