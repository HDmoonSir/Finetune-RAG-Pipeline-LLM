import os
import json
import glob
import google.generativeai as genai
from tqdm import tqdm
import math
import argparse
import typing as tp
import config

# 1. Gemini API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
try:
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY ÌôòÍ≤Ω Î≥ÄÏàòÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    genai.configure(api_key=api_key)
except (ValueError, KeyError) as e:
    print(f"API ÌÇ§ ÏÑ§Ï†ï Ïò§Î•ò: {e}")
    exit()

def generate_qa_pairs(prompt_content: str) -> tp.Optional[tp.List[tp.Dict[str, str]]]:
    """Gemini Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Q&A ÏåçÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§."""
    model = genai.GenerativeModel(
        config.GEMINI_PREPROCESS_MODEL,
        generation_config=genai.types.GenerationConfig(response_mime_type="application/json")
    )
    try:
        response = model.generate_content(prompt_content)
        json_text = response.text.strip()
        if json_text.startswith("```json"):
            json_text = json_text[7:-4].strip()
        json_data = json.loads(json_text)
        return json_data.get('qa_pairs', [])
    except (json.JSONDecodeError, AttributeError, ValueError) as e:
        print(f"-- Q&A ÏÉùÏÑ± Ïò§Î•ò: {e}")
        return None
    except Exception as e:
        print(f"-- ÏòàÍ∏∞Ïπò ÏïäÏùÄ Ïò§Î•ò Î∞úÏÉù: {e}")
        return None

def generate_unsupervised_text(prompt_content: str) -> tp.Optional[str]:
    """Gemini Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎπÑÏßÄÎèÑ ÌïôÏäµÏö© ÌÖçÏä§Ìä∏Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§."""
    model = genai.GenerativeModel(config.GEMINI_PREPROCESS_MODEL)
    try:
        response = model.generate_content(prompt_content)
        return response.text
    except Exception as e:
        print(f"-- ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Ïò§Î•ò: {e}")
        return None

def format_for_llama3(qa_pairs: tp.List[tp.Dict[str, str]]) -> tp.List[tp.Dict[str, str]]:
    """Q&A ÏåçÏùÑ Llama 3Ïùò Instruction Fine-tuning ÌòïÏãùÏúºÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§."""
    formatted_data = []
    for pair in qa_pairs:
        if 'question' not in pair or 'answer' not in pair or not pair['question'] or not pair['answer']:
            continue
        instruction = pair['question']
        response = pair['answer']
        formatted_string = (
            f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
            f"{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            f"{response}<|eot_id|>")
        formatted_data.append({"text": formatted_string})
    return formatted_data

def main(mode: str) -> None:
    print(f"üöÄ'{mode}' Î™®ÎìúÎ°ú Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±ÏùÑ ÏãúÏûëÌï©ÎãàÎã§.")

    jsonl_paths: tp.List[str] = glob.glob(os.path.join(config.DATA_DIR, "**/*.jsonl"), recursive=True) + glob.glob(os.path.join(config.DATA_DIR, "*.jsonl"))
    jsonl_paths = sorted(list(set(jsonl_paths)))

    if not jsonl_paths:
        print("Ï≤òÎ¶¨Ìï† JSONL ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.")
        return

    all_pages_by_pdf: tp.Dict[str, tp.List[tp.Dict[str, tp.Any]]] = {}
    for path in jsonl_paths:
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data: tp.Dict[str, tp.Any] = json.loads(line)
                    source_pdf: tp.Optional[str] = data.get('source_pdf')
                    if source_pdf:
                        if source_pdf not in all_pages_by_pdf:
                            all_pages_by_pdf[source_pdf] = []
                        all_pages_by_pdf[source_pdf].append(data)
                except json.JSONDecodeError:
                    print(f"‚ö†Ô∏è Warning: {path} ÌååÏùºÏùò ÏùºÎ∂Ä ÎùºÏù∏ÏóêÏÑú JSON ÎîîÏΩîÎî© Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.")

    for pdf in all_pages_by_pdf:
        all_pages_by_pdf[pdf].sort(key=lambda x: x.get('page_number', 0))

    all_generated_data: tp.List[tp.Any] = []
    total_api_calls: int = 0
    TEXT_PAGE_BATCH_SIZE: int = config.TEXT_PAGE_BATCH_SIZE

    qa_prompt_template: str = config.QA_PROMPT_TEMPLATE
    unsupervised_prompt_template: str = config.UNSUPERVISED_PROMPT_TEMPLATE

    for pdf_name, pages in all_pages_by_pdf.items():
        print(f"\nüìÑ {pdf_name} Î¨∏ÏÑú Ï≤òÎ¶¨ Ï§ë...")
        page_texts: tp.List[str] = [p['text'] for p in pages if p.get('text') and len(p['text'].strip()) > 50]
        if not page_texts:
            continue

        num_batches: int = math.ceil(len(page_texts) / TEXT_PAGE_BATCH_SIZE)
        for i in tqdm(range(num_batches), desc=f"{os.path.basename(pdf_name)} Ï≤òÎ¶¨ ÏßÑÌñâÎ•†"):
            batch_texts: tp.List[str] = page_texts[i * TEXT_PAGE_BATCH_SIZE : (i + 1) * TEXT_PAGE_BATCH_SIZE]
            combined_text: str = "\n\n---\n\n".join(batch_texts)

            if mode == 'qa':
                prompt: str = qa_prompt_template.format(text=combined_text)
                result: tp.Optional[tp.List[tp.Dict[str, str]]] = generate_qa_pairs(prompt)
                if result:
                    all_generated_data.extend(result)
            else: # unsupervised
                prompt = unsupervised_prompt_template.format(text=combined_text)
                result: tp.Optional[str] = generate_unsupervised_text(prompt)
                if result:
                    all_generated_data.append({"text": result})
            
            total_api_calls += 1

    print(f"\nTotal API calls made: {total_api_calls}")
    print(f"Total items generated before processing: {len(all_generated_data)}")

    if mode == 'qa':
        unique_qa_pairs: tp.List[tp.Dict[str, str]] = []
        processed_questions: tp.Set[str] = set()
        for pair in all_generated_data:
            question: tp.Optional[str] = pair.get('question')
            if question and question not in processed_questions:
                unique_qa_pairs.append(pair)
                processed_questions.add(question)
        final_data: tp.List[tp.Dict[str, str]] = format_for_llama3(unique_qa_pairs)
        output_file: str = config.QA_DATASET_PATH
        print(f"Total unique Q&A pairs after deduplication: {len(unique_qa_pairs)}")
    else:
        final_data = all_generated_data
        output_file = config.UNSUPERVISED_DATASET_PATH

    os.makedirs(config.DEFAULT_OUTPUT_DIR, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        for item in final_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"\n‚úÖ All data has been saved to '{output_file}'.")
    print(f"Total final records created: {len(final_data)}")
